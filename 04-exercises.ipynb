{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVhqWh3jyl7N"
   },
   "source": [
    "# DSCI 511: Data acquisition and pre-processing<br>Chapter 4: Pre-processing considerations: foresight for downstream needs\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes.\n",
    "\n",
    "#### 4.1.1.1 Exercise: CSV to JSON conversion\n",
    "Read the `cities.csv` file and look at its contents. It should have a header (the first line of the file) that tells you which fields contain what data. Next, take the data for  only the cities which have their population listed and store this in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02ghPRMAWhrm"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Yi6UqMyl7f"
   },
   "source": [
    "#### 4.1.2.1 Exercise: JSON to CSV conversion\n",
    "Load the data in the `american-movies.json` file. We only want the movies that were made from 1990 to 1999 (it was a truly glorious decade for American cinema). Your task is to take the title and year of making for these movies and put these in a tab-separated values file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRo8K2FAWmcC"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PB0LAjcyl75"
   },
   "source": [
    "#### 4.1.2.4 Exercise: Making JSON file reading scalable\n",
    "Create a specialized JSON serialization of the data in `'nobel-laureates.json'`. Specifically, create a file called `'data/nobel-laureates-lines.json'` that has each lauriate's record serialized seprately as a json object, with newlines `'\\n'` in between, as delimiters. As a follow up, combine the line-by-line file reading syntax introduced in Section 1.4.1.5 in conjunction with the `json.dumps()` string serialization function in Section 1.4.2.2 to _read only the first ten lines_. As you read these lines, load each from json and print the laureate's list of prizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKPUQRB5Wsgm"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XodggAQ5yl8M"
   },
   "source": [
    "#### 4.4.1.3 Exercise: Regex phone numbers\n",
    "Read the file `phone-numbers.txt`. It contains a phone number in each line. \\[Hint: use something like `lines = open(\"file.txt\", \"r\").readlines()`\\] Store only the phone numbers with the area code \"215\" in a list and print it out. Use regex-based pattern matching, not any other methods which occur to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3eXn3H7XK1f"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX1FFsMnyl8U"
   },
   "source": [
    "#### 4.4.1.8 Exercise: Names of the gods\n",
    "In the cell below is some text. It's an extract from [A Clash of Kings](https://www.goodreads.com/book/show/10572.A_Clash_of_Kings), specifically, about a character's prayer to some fictional gods. Use regex to extract the names of these gods. Your output should be a list that looks something like `[\"the Father\", \"the Mother\", \"the Warrior\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GV4D4hPQXQwr"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Sdo2uBcyl8c"
   },
   "source": [
    "#### 4.4.4.2 Exercise: Calculate youre exact age\n",
    "Calculate your own age using datetime parsing! Can you come up with a datetime format for your birthday that `dateutil.parser` doesn't recognize or recognizes incorrectly? If so, use the `datetime` module to specify the format exactly. [Hint. Review these docs: \n",
    "- https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\n",
    "- https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXGwZ3WYXWkU"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3LXUr4Dyv6l"
   },
   "source": [
    "## Additional In-depth Exercises\n",
    "### A. Batching Twitter Replies Requests\n",
    "At the following path:\n",
    "\n",
    "- ./data/1254014899950366720.json\n",
    "\n",
    "we have data represents a 'thread' initiated by Elon Musk's tweet:\n",
    "\n",
    "- https://twitter.com/elonmusk/status/1254014899950366720\n",
    "\n",
    "we'll be working with this thread through a few acquisition-related pre-processing exercises throughout, so here's a data load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMMB2eFmyv6m",
    "outputId": "ad6e3804-2715-4789-c7ae-9b49df51d4ae",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['thread', 'tweets', 'rpws']),\n",
       " {'created_at': 'Sat Apr 25 23:56:07 +0000 2020',\n",
       "  'id': 1254197528607637505,\n",
       "  'id_str': '1254197528607637505',\n",
       "  'full_text': '@elonmusk Looks* sheesh... algorithms hahaha',\n",
       "  'truncated': False,\n",
       "  'display_text_range': [10, 44],\n",
       "  'entities': {'hashtags': [],\n",
       "   'symbols': [],\n",
       "   'user_mentions': [{'screen_name': 'elonmusk',\n",
       "     'name': 'Elon Musk',\n",
       "     'id': 44196397,\n",
       "     'id_str': '44196397',\n",
       "     'indices': [0, 9]}],\n",
       "   'urls': []},\n",
       "  'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       "  'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>',\n",
       "  'in_reply_to_status_id': 1254179739620655105,\n",
       "  'in_reply_to_status_id_str': '1254179739620655105',\n",
       "  'in_reply_to_user_id': 1165558118262030343,\n",
       "  'in_reply_to_user_id_str': '1165558118262030343',\n",
       "  'in_reply_to_screen_name': 'kohtakangas',\n",
       "  'user': {'id': 1165558118262030343,\n",
       "   'id_str': '1165558118262030343',\n",
       "   'name': 'Misty Brooke Martin',\n",
       "   'screen_name': 'kohtakangas',\n",
       "   'location': '',\n",
       "   'description': 'I suppose where the animus of Jung and Tesla finally collide into one. One in a million really. A unicorn! hahaha',\n",
       "   'url': None,\n",
       "   'entities': {'description': {'urls': []}},\n",
       "   'protected': False,\n",
       "   'followers_count': 63,\n",
       "   'friends_count': 249,\n",
       "   'listed_count': 0,\n",
       "   'created_at': 'Sun Aug 25 09:34:55 +0000 2019',\n",
       "   'favourites_count': 2352,\n",
       "   'utc_offset': None,\n",
       "   'time_zone': None,\n",
       "   'geo_enabled': False,\n",
       "   'verified': False,\n",
       "   'statuses_count': 4768,\n",
       "   'lang': None,\n",
       "   'contributors_enabled': False,\n",
       "   'is_translator': False,\n",
       "   'is_translation_enabled': False,\n",
       "   'profile_background_color': 'F5F8FA',\n",
       "   'profile_background_image_url': None,\n",
       "   'profile_background_image_url_https': None,\n",
       "   'profile_background_tile': False,\n",
       "   'profile_image_url': 'http://pbs.twimg.com/profile_images/1254113492807716865/ZGC6AK1j_normal.jpg',\n",
       "   'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1254113492807716865/ZGC6AK1j_normal.jpg',\n",
       "   'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1165558118262030343/1582844472',\n",
       "   'profile_link_color': '1DA1F2',\n",
       "   'profile_sidebar_border_color': 'C0DEED',\n",
       "   'profile_sidebar_fill_color': 'DDEEF6',\n",
       "   'profile_text_color': '333333',\n",
       "   'profile_use_background_image': True,\n",
       "   'has_extended_profile': True,\n",
       "   'default_profile': True,\n",
       "   'default_profile_image': False,\n",
       "   'following': None,\n",
       "   'follow_request_sent': None,\n",
       "   'notifications': None,\n",
       "   'translator_type': 'none'},\n",
       "  'geo': None,\n",
       "  'coordinates': None,\n",
       "  'place': None,\n",
       "  'contributors': None,\n",
       "  'is_quote_status': False,\n",
       "  'retweet_count': 0,\n",
       "  'favorite_count': 0,\n",
       "  'favorited': False,\n",
       "  'retweeted': False,\n",
       "  'lang': 'en'})"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.loads(open(\"./data/1254014899950366720.json\").read())\n",
    "data.keys(), data['tweets'][\"1254197528607637505\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djOBDXHdyv6v"
   },
   "source": [
    "The three keys in this dictionary correspond to the \n",
    "1. `thread`'s topology, organized as a heirarchy of nested dictionaries, keyed by tweet ids,\n",
    "2. conversation's `tweets`, themselves, keyed in a dictionary by their own tweet ids, and\n",
    "3. a log of the current average <i>replies per week</i> (`rpw`) observed in a separate, replies database for each user. \n",
    "\n",
    "Here's a more programmatic view of the overall schema:\n",
    "\n",
    "```\n",
    "data = {\n",
    "    'thread': {\n",
    "        source: {\n",
    "            child_1: {\n",
    "                child_1_1: {...},\n",
    "                ...\n",
    "            },\n",
    "            child_2: {...},\n",
    "            ...\n",
    "        }\n",
    "    },\n",
    "    'rpws': {\n",
    "        user_id: [rpw, max_time],\n",
    "        ...\n",
    "    },\n",
    "    'tweets': {\n",
    "        tweet_id: tweet,\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "where `max_time` refers to the most recent time stamp of any tweet in the `thread`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ife1AY0nyv6y"
   },
   "source": [
    "#### A.1 Exercise: Working with twitter timestamps\n",
    "Utilise the datetime module and determine a string, `ttime`, which expresses the Twitter `'created_at'` date string's format to the datetime module, and then use datetime to parse any timestamp of your choosing in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2kkMsAxXiXq"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6xNL7xMyv69"
   },
   "source": [
    "#### A.2 Exercise: Penn or Drexeluniv Who get's more replies per API batch: Penn or Drexeluniv?\n",
    "This problem focuses on query batching on Twitter's search API. To get started, review the search API standard operators, and determine which (__at least 2 operators__) we'll need to be able to 1) filter for user replies 2) query for replies targeting two users at the same time. Here's the docs:\n",
    "\n",
    "- https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators\n",
    "\n",
    "For Twython reference, please review:\n",
    "\n",
    "- https://twython.readthedocs.io/en/latest/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01dPupFAXnct"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ohtick8Wyv7I"
   },
   "source": [
    "#### A.3 Exercise: Querying Twitter's rate limits, and working with \"unix time\"\n",
    "Now that we've checked on some replies to two Twitter users, let's check to see when our rate limit will be refreshed. In particular, use the `twitter.get_lastfunction_header(header_name)` method to recover each, by `header_name`:\n",
    "- `'x-rate-limit-limit'` \n",
    "- `'x-rate-limit-remaining'`\n",
    "- `'x-rate-limit-class'`\n",
    "- `'x-rate-limit-reset'`\n",
    "\n",
    "For information about the header-request method, see the Twython docs for details:\n",
    "\n",
    "- https://twython.readthedocs.io/en/latest/api.html\n",
    "\n",
    "Once you've collected the headers, convert the value of `'x-rate-limit-reset'` to a datetime object, and compare it to four hours past the current EST time (using `dt.now()`), to account for the time shift with Tweets, which are expressed in GMT. When this is complete exhibit both.\n",
    "\n",
    "[Hint: here's a relevant stackoverflow post about this datetime coversion in Python: https://stackoverflow.com/questions/7703865/going-from-twitter-date-to-python-datetime-date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qJ42xUMXt6l"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPLOvAjgyv7S"
   },
   "source": [
    "#### A.4 Exercise: Maximizing query batch size for Twitter's rate limits\n",
    "Now that we've vetted how to make batch requests for replies our next job is to determine how many we can combine into a single query&mdash;__is this always the same for each query?__\n",
    "\n",
    "To answer this question, first consult the restrictions on the query parameter, `q`:\n",
    "\n",
    "- https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "\n",
    "__In particular, how many characters can we utilize per query, and how can we construct queries coming as close as possible to this maximum?__\n",
    "\n",
    "When you've determined your strategy, build as many batches as is required from the sorting order of user ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ruib56GxXySu"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7t6NpD2yv7n"
   },
   "source": [
    "#### A.5 Exercise: Evaluating query batch size for Twitter's rate limits\n",
    "Now that we know how to build queries that request replies for the most users at once, our next goal is to find a way to batch users together into queries in a way that balances their agreegated numbers of replies per week. So the first thing we'll need is to have a way of evaluating the API rate-limit 'burden' of a given query-batch of users.\n",
    "\n",
    "So, to get this off of the ground, put together code that determines the number of replies per week per batch constructed in __A.4__ In particular, functionalize your code so that the aggregated replies per week can be assessed for an arbitrary batch in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyhOJwUqyv7p"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw3O_g0l2RhH"
   },
   "source": [
    "### B. Structuring Reddit Threads\n",
    "\n",
    "Run the script in the cell below to build the reddit data object `data`. We'll be using these throughout the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B14edCF2TRC"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_data(sub_id):\n",
    "  post_url = \"https://api.pushshift.io/reddit/search/submission/?ids=\" + sub_id\n",
    "  post = requests.get(post_url)\n",
    "  post_resp = requests.get(post_url)\n",
    "  post = post_resp.json()\n",
    "\n",
    "  data = [post['data'][0]]\n",
    "\n",
    "  comments_url = \"https://api.pushshift.io/reddit/submission/comment_ids/\" + sub_id\n",
    "  comments_resp = requests.get(comments_url)\n",
    "  ids = comments_resp.json()\n",
    "\n",
    "  batch_size = 500\n",
    "  for batch_num in range(len(ids['data'])//batch_size):\n",
    "    url = \"https://api.pushshift.io/reddit/comment/search?ids=\" + ','.join(ids['data'][batch_size*batch_num:batch_size*(batch_num + 1)])\n",
    "    resp = requests.get(url)\n",
    "    batch = resp.json()\n",
    "    data.extend(batch['data'])\n",
    "\n",
    "  if len(data) != len(ids['data']) + 1:\n",
    "    url = \"https://api.pushshift.io/reddit/comment/search?ids=\" + ','.join(ids['data'][len(data):])\n",
    "    resp = requests.get(url)\n",
    "    batch = resp.json()\n",
    "    data.extend(batch['data'])\n",
    "\n",
    "  return(data)\n",
    "\n",
    "sub_id = \"j1dynm\"\n",
    "data = get_data(sub_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElU19pl14j45"
   },
   "source": [
    "#### B.1 Exercise: Reviewing the Reddit comment data structure\n",
    "Let's just take 5 minutes to review `data` and determine the following:\n",
    "- What is the overall object type?\n",
    "- What does a single element (comment) look like? (think schema)\n",
    "- How do these data connect together, i.e., where's the 'thread'?\n",
    "- Are the data ordered by time, and if not how could they be? \n",
    "\n",
    "Write any responses to these questions that you determine in the response box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cubQwqeW5PMy"
   },
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrVQ_DqVYup9"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVc2IX3I5Ub_"
   },
   "source": [
    "#### B.2 Exercise: Fast access by comment id\n",
    "If we want to be able to quickly interact between comment, a convenient option would be to re-format into a dictionary. In particular, consutrct a `dict` called `comments` from `data` that is of the format:\n",
    "\n",
    "```\n",
    "comments = {\n",
    "  id: comment,\n",
    "  ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bewFzFbjYyrB"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMq0fNqH7ZO-"
   },
   "source": [
    "#### B.3 Exercise: De-serializing timestamps\n",
    "Now that we have our data set up for fast access, let's see if we can `activate` the timestamps, keyed by the `'created_utc'` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x85CCkis7K5m"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBGwgx0E76i3"
   },
   "source": [
    "#### B.4 Exercise: Computing timedeltas\n",
    "Now that we have the ability to compute over Reddit's time stamps, let's create a function that takes a comment and determines the number of seconds that have elapsed in between the comment and its parent post, keyed by `'parent_id'`. For this, utilize the `timedelta` function within the `datetime` module and consider how to make linkages between comments and the post (top-level), vs. comments and other comments (replies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C91BWg7776i8"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "## code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
