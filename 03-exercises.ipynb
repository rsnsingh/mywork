{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcp5U6_bs-Kd"
   },
   "source": [
    "# DSCI 511: Data acquisition and pre-processing<br>Chapter 3: Acquiring Data from the Internet\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldorqzbps-Kf"
   },
   "source": [
    "#### 3.1.2.3 Exercise: processing a JSON response\n",
    "Make a request to the SEPTA Arrivals API to get data on the next 10 trains to arrive at Suburban Station. Store this JSON-format data into a dictionary. Inspect the dictionary structure. Then, write code to create a list containing 10 dictionaries, one for each train. These new dictionaries should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBfxKBZIQRGd"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tp36iJbPs-Ky"
   },
   "source": [
    "#### 3.2.1.1 Exercise: accessing a soccer schedule\n",
    "\n",
    "Make a request to the Sportradar Soccer schedule API to obtain the match schedule for Liverpool FC (team_id = sr:competitor:44). Then, from the obtained schedule, make a simple list of fixtures. Your output should be a list with strings as elements. The strings should be of the format \"HOME_TEAM vs AWAY_TEAM\".\n",
    "\n",
    "- https://developer.sportradar.com/docs/read/Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQkvek_0U5SG"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9wDmM_is-K_"
   },
   "source": [
    "#### 3.3.3.1 Exercise: access some accidental haikus from Twitter's REST API\n",
    "Create your Twitter API keys and download the last 15 tweets by @accidental575 (the hilarious Accidental Haiku Bot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XckuFQolVGAo"
   },
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0k9lmofLs-LJ"
   },
   "source": [
    "## Additional In-depth Exercises\n",
    "### A. Building Twitter reply chains\n",
    "Since Twitter provides no direct functionality for downloading reply chains, alternative strategies are needed. Some good suggestions can be found here on github:\n",
    "\n",
    "- https://github.com/alok/thread-twitter/blob/master/thread_tweets.py\n",
    "- https://gist.github.com/edsu/54e6f7d63df3866a87a15aed17b51eaf\n",
    "\n",
    "and the latter provides some excellent explanation for this development need:\n",
    "\n",
    "> Twitter's API doesn't allow you to get replies to a particular tweet. Strange\n",
    "but true. But you can use Twitter's Search API to search for tweets that are\n",
    "directed at a particular user, and then search through the results to see if \n",
    "any are replies to a given tweet. You probably are also interested in the\n",
    "replies to any replies as well, so the process is recursive. The big caveat \n",
    "here is that the search API only returns results for the last 7 days. So \n",
    "you'll want to run this sooner rather than later.\n",
    "\n",
    "Apply these techniques using Twython, by searching over source-user mentions from either of the following endpoints:\n",
    "\n",
    "- https://developer.twitter.com/en/docs/tweets/search/overview/standard\n",
    "- https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-mentions_timeline\n",
    "\n",
    "to exhibit how replies to a given message are obtained. To continue, store the tweets one tweet per line in a 'json lines' file, and organize the thread's Tweet IDs in their natural, hierarchical graph/network structure by utilizing dictionaries and json serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1mbmUNcs-MQ"
   },
   "source": [
    "### B. Explore Reddit's pushshift api:\n",
    "Reddit's API's aren't quite as clear and comprehensive as Twitters and as result, the best archive I've seen of complete historical Reddit data is hosted by a third party:\n",
    "\n",
    "- https://github.com/pushshift/api\n",
    "- https://pushshift.io/api-parameters/\n",
    "\n",
    "Let's plan as above to explore this (unauthenticated) API, focused with the intention of mining out comment threads. \n",
    "\n",
    "Of particular use to us will be the referenced call to Get all Comment IDs for a particular Submission:\n",
    "\n",
    "- https://github.com/pushshift/api#get-all-comment-ids-for-a-particular-submission\n",
    "\n",
    "in addition to the endpoint which will allow us to get all comments from a comma-separated list of ids:\n",
    "\n",
    "- https://github.com/pushshift/api#getting-comments-based-on-id\n",
    "\n",
    "Let's work from here, considering the following submission (post) id: `j1dynm`.\n",
    "\n",
    "Note: we should take extra care to determine if these submission ids are complete, or just the direct replies to the original submission. If they're only these 'top level' replies, then we'll have to approach via the same methods as the Twitter API, recursively or iteratively searching for replies to individual comment objects. If not, then we're in excellent shape to begin requesting the comment objects. Regardless, we may have to adhere to the API's `size` parameter which accepts values $\\leq500$, so we should be a little bit cautious here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKtgNDUVbTn6"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hUQKu1C09Kc"
   },
   "source": [
    "### C. Investigate the ProPublica API\n",
    "\n",
    "As an additional exercise we'll investigate the availability of congressional data from ProPublica's API:\n",
    "\n",
    "- https://www.propublica.org/datastore/api/propublica-congress-api\n",
    "\n",
    "You'll have to request an api key, which you can do via the link above. To pass our key through the api, we'll have to use a header, in a format like:\n",
    "\n",
    "```\n",
    "requests.get(URL, headers={'X-API-Key': key})\n",
    "```\n",
    "\n",
    "As a target, let's see if we can find the transcript of the recent congressional hearing from the House Committee on the Judiciary:\n",
    "\n",
    "- https://www.congress.gov/event/116th-congress/house-event/110883\n",
    "- https://docs.house.gov/Committee/Calendar/ByEvent.aspx?EventID=110883\n",
    "- https://docs.house.gov/Committee/Calendar/ByDay.aspx?DayID=07292020\n",
    "\n",
    "We can start by looking for our committee on the list provided by the following endpoint:\n",
    "\n",
    "- https://projects.propublica.org/api-docs/congress-api/committees/#lists-of-committees\n",
    "\n",
    "From this we'll be able to determine our specific committee's id, and then use the endpoint for hearing from a specific committee:\n",
    "\n",
    "- https://projects.propublica.org/api-docs/congress-api/committees/#get-hearings-for-a-specific-committee\n",
    "\n",
    "If it turns out our hearing isn't listed, we might be able to check and see how up to date the data are, by looking at what hearings are 'recent':\n",
    "\n",
    "- https://projects.propublica.org/api-docs/congress-api/committees/#get-recent-committee-hearings\n",
    "\n",
    "#### Taking a look at the committees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48vv7ecRbfPP"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
