{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCaoK6JsbX0W"
   },
   "source": [
    "# DSCI 511: Data acquisition and pre-processing<br>Chapter 5: Harvesting Content from the World Wide Web\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0U-niAnXbX0Z"
   },
   "source": [
    "#### 5.2.1.0 Example: Parsing a BeautifulSoup document\n",
    "Access the content from `'http://catalog.drexel.edu/'`, and find the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0V8RE219bqIx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Drexel University &lt; 2022-2023 Catalog | Drexel University\n",
      "<p>In order to graduate, all students must pass three writing-intensive courses after their freshman year. Two writing-intensive courses must be in a student's major. The third can be in any discipline. Students are advised to take one writing-intensive class each year, beginning with the sophomore year, and to avoid “clustering” these courses near the end of their matriculation. Transfer students need to meet with an academic advisor to review the number of writing-intensive courses required to graduate.</p>\n",
      "\t\t<p>For additional information, and an up-to-date list of the writing-intensive courses being offered, students should check the Drexel University Writing Center page\n"
     ]
    }
   ],
   "source": [
    "## code here\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# format: \"https://api.github.com/user/USERNAME\"\n",
    "response = requests.get(\"https://catalog.drexel.edu/\")\n",
    "list = response.text[:]\n",
    "s= list.find(\"<title>\")\n",
    "e= list.find(\"</title>\")\n",
    "print(list[s:e])\n",
    "\n",
    "#s= list.find(\"<body>\")\n",
    "#e= list.find(\"</body>\")\n",
    "#print(list[s:e])\n",
    "\n",
    "s= list.find(\"<p>\")\n",
    "e= list.rfind(\"</p>\")\n",
    "print(list[s:e])\n",
    "#pprint(response.text[:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPHKws_UbX0x"
   },
   "source": [
    "#### 5.2.1.3 Understanding the body\n",
    "Find and print the body content of the University Catalog (`'http://catalog.drexel.edu/'`) and review the structure of the html. What might get us into the actual catalog data? Can you find the code for the Undergraduate and Graduate 'buttons'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH_2ncE0bsLx"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGTUWCG0bX0_"
   },
   "source": [
    "#### 5.2.1.5 Exercise: Examining tag attributes\n",
    "Find all `'p'`-tagged blocks in the University Catalog (`'http://catalog.drexel.edu/'`) and examine any tag attributes. What sort of information is being conveyed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGvgVjpzcBqd"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-Z5fsUwbX1T"
   },
   "source": [
    "#### 5.2.1.6 Exercise: Filtering tags by attribute\n",
    "Returning to `'http://catalog.drexel.edu/'`, can you collect only those hyperlinks which don't have the attribute `target=\"_blank\"`? \\[__Hint__: Think sets and exclusion.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PfYnsU5cEfo"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyu5f3E4bX1n"
   },
   "source": [
    "#### 5.3.1.3 Extracting hyperlinks for a crawl\n",
    "From the Graduate Quarter course descriptions page (`'http://catalog.drexel.edu/coursedescriptions/quarter/grad/'`), extract a list of URLs to the various program course descriptions pages.  Can you keep them  grouped by academic unit? \\[__Hint__: To do so, you can use the page layout regular expressions, i.e., `re.compile()` to manage the unit headings.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7Icb6gScHIm"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhYSLyjKbX12"
   },
   "source": [
    "#### 5.3.2.9 Extracting the data\n",
    "Write a function that scrapes the course descriptions from a given program page. For example, Drexel's MSDS course descriptions page is `'http://catalog.drexel.edu/coursedescriptions/quarter/grad/dsci/'`. Retain as much structure as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJD27bzucKmY"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iboQm9tew2Uf"
   },
   "source": [
    "## Additional In-depth Exercises\n",
    "### A. Scoping a fact checking site\n",
    "Here, our __overall goal__ will be to build scraper that can pull the content out of a fact-checking site, so that we can determine at any pint the relative truthfullness of a political figure, by request.\n",
    "\n",
    "#### A.1 Review the terms and conditions of content use\n",
    "First, navigate to https://www.politifact.com and determine what options exist for web scraping its content.\n",
    "\n",
    "[__Hint__: review the Terms, conditions and copyright statement: https://www.politifact.com/copyright/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dpDgQOWw2Uh"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHTwG_Wxw2Uo"
   },
   "source": [
    "#### A.2 How can we get access to a list of all people?\n",
    "Reivew the main site again (https://www.politifact.com)&mdash;how can we get a list of all the available people, so that we can see if the user's request exists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 4057,
     "status": "ok",
     "timestamp": 1603136851564,
     "user": {
      "displayName": "Dr. Jake",
      "photoUrl": "",
      "userId": "10996578946780976051"
     },
     "user_tz": 240
    },
    "id": "vmy-x4t6ZsoX",
    "outputId": "84cb7a84-aa73-41af-db20-79a001269e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytDWLvupmqas"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckvvcxIHk8HE"
   },
   "source": [
    "#### A.3 Understanding the target data\n",
    "Now that we have the html for the collection of 'personalities' represented on PolitiFact the job is to scrape them. First, review the available data from the html to determine which data should be stored for the personalities represented, and sketch a schema in the response box below for the entire collection of data on personalities represented on the page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPJdVdTPlsYH"
   },
   "source": [
    "```\n",
    "sketch an object scheme here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjDwv9Zlio3"
   },
   "source": [
    "#### A.4 Scraping the list of 'personalities'\n",
    "Now that we have the html for the collection of 'personalities' represented on PolitiFact the job is to scrape them. First, review the available data from the html to determine which data should be stored on the personalities represented and sketch a schema in the response box below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4hVn1K9mtYj"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia_bYiqdw2U2"
   },
   "source": [
    "#### A.5 Scrape the content for a given individual.\n",
    "Now, navigate to an individual's page of your choosing, and sort out a web-scraping process that recovers all available data about them in as nicely-structured way as possible.\n",
    "\n",
    "Interestingly, it appears that one of the individuals (personalities) present in the data is 'Facebook Posts'. As you test your code for working on a single 'individual', i.e., utilize: \n",
    "- `url = 'https://www.politifact.com/personalities/facebook-posts/'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckFNSEqRw2U3"
   },
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8953bee629eee860cd4e04df000094913af0962207d7a23e50f22b229dacbf40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
